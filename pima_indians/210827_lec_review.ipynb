{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns; sns.set()\r\n",
    "import warnings; warnings.filterwarnings(action='ignore')\r\n",
    "\r\n",
    "# load datasets\r\n",
    "from sklearn import datasets\r\n",
    "\r\n",
    "# model_selection.splitter\r\n",
    "from sklearn.model_selection import train_test_split # function\r\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\r\n",
    "\r\n",
    "# model_selection.hyper_parameter_optimizer\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "# model_selection.model_validation\r\n",
    "from sklearn.model_selection import cross_val_predict\r\n",
    "\r\n",
    "# models/estimators\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "# preprocessing.encoding\r\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # pd.get_dummies()\r\n",
    "\r\n",
    "# preprocessing.scaling\r\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\r\n",
    "\r\n",
    "# preprocessing.binarizing\r\n",
    "from sklearn.preprocessing import Binarizer\r\n",
    "\r\n",
    "# metrics.scores\r\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\r\n",
    "from sklearn.metrics import f1_score, roc_auc_score\r\n",
    "\r\n",
    "# metrics.curves\r\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\r\n",
    "from sklearn.metrics import roc_curve, plot_roc_curve\r\n",
    "\r\n",
    "# metrics.reports\r\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\r\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# read pima diabetes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./kaggle/pima/diabetes.csv')\r\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X, y"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df.drop('Outcome', axis=1)\r\n",
    "y = df['Outcome']\r\n",
    "model = RandomForestClassifier(n_estimators=500, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# preprocessing\r\n",
    "\r\n",
    "전처리 ; 모두 fit/transform 있음, 아래 6개만 사용\r\n",
    "\r\n",
    "1. encoder   ; LabelEncoder, OneHotEncoder\r\n",
    "2. scaler    ; MinMaxScaler, StandardScaler, RobustScaler\r\n",
    "3. bizarizer ; Binarizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LabelEncoder\r\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\n",
    "\r\n",
    "data = ['a', 'b', 'b', 'c', 'd']\r\n",
    "\r\n",
    "encoder = LabelEncoder()    # 사전 순으로 sort labeling\r\n",
    "encoder.fit(data)           # make labels, return self\r\n",
    "encoder.transform(data)     # apply labels, return arr\r\n",
    "encoder.fit_transform(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# srs.map(dict) ; 특정 값\r\n",
    "mapping = {'a':1, 'b':0, 'c':2}\r\n",
    "map_df = pd.DataFrame(data, columns=['chars'])\r\n",
    "map_df['chars_map'] = map_df['chars'].map(mapping)\r\n",
    "map_df[['chars', 'chars_map']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MinMaxScaler ; 최솟값, 최댓값 기준으로 정규화\r\n",
    "# StandardScaler ; mean=0, unit_variance(편차)=1로 정규화\r\n",
    "# RobustScaler ; median(중간값)을 기준으로 4등분하여 정규화\r\n",
    "\r\n",
    "# classification tree model은 scaling 영향 적게 받음\r\n",
    "# regression model은 scaling 영향 크게 받음\r\n",
    "\r\n",
    "s_train = np.array(list(range(0, 10))).reshape(-1, 1)\r\n",
    "s_test  = np.array(list(range(0,  5))).reshape(-1, 1)\r\n",
    "\r\n",
    "# train scaling\r\n",
    "scaler_f = MinMaxScaler()\r\n",
    "scaler_t = MinMaxScaler()\r\n",
    "scaler_f.fit(s_train)                 # input arr, return self\r\n",
    "scaler_t.fit(s_train) \r\n",
    "train_f = scaler_f.transform(s_train) # apply fit, return arr\r\n",
    "train_t = scaler_t.transform(s_train)\r\n",
    "\r\n",
    "# test scaling ; test는 fit하면 안 됨\r\n",
    "scaler_f.fit(s_test)\r\n",
    "test_f = scaler_f.transform(s_test)\r\n",
    "\r\n",
    "test_t = scaler_t.transform(s_test)\r\n",
    "\r\n",
    "print('----- train scaling  -----\\n', train_f.T)\r\n",
    "print('----- test fit & trf -----\\n', test_f.T)\r\n",
    "print('----- test transform -----\\n', test_t.T)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## binarize\r\n",
    "\r\n",
    "- threshold ; N/P를 나누는 기준 확률점 (임계치, 임계점), predict의 default는 0.5 초과\r\n",
    "- binarize ; threshold를 기준으로 N=0, P=1로 이진화\r\n",
    "- binarizing ; threshold를 조정하여, N/P의 비율을 조정\r\n",
    "- 일반적으로 P의 비율이 적기 때문에, threshold를 낮춰서 점수 개선\r\n",
    "- oversampling ; P가 적을 경우, 인위적으로 P 데이터의 절대적인 양을 늘리는 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# threshold 조정에 따른 N/P 변화\r\n",
    "\r\n",
    "# threshold=2 일 때, P=5 N=4\r\n",
    "T = [[-1, 1, 0],\r\n",
    "     [ 1, 2, 3],\r\n",
    "     [ 2, 3, 4]]\r\n",
    "Binarizer(threshold=1).fit_transform(X) # input predict_proba, return arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# threshold=3 일 때, P=1 N=8\r\n",
    "Binarizer(threshold=3).fit_transform(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# scoring & plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pr_curve(y_val, probas_pred):\r\n",
    "\r\n",
    "    \"\"\"threshold / precision, recall, f1 curve\"\"\"\r\n",
    "\r\n",
    "    # plot_precision_recall_curve(model, X_val, y_val) # x=recall, y=precision\r\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_val, probas_pred[:, 1])\r\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\r\n",
    "\r\n",
    "    # settings\r\n",
    "    plt.title('precision recall f1 curve')\r\n",
    "    plt.gray()\r\n",
    "    plt.xlabel('threshold')\r\n",
    "    plt.ylabel('socre')\r\n",
    "\r\n",
    "    # x, y values\r\n",
    "    plt.plot(thresholds, precisions[:thresholds.shape[0]], label='precision', linestyle=':')\r\n",
    "    plt.plot(thresholds, recalls[:thresholds.shape[0]],    label='recall',    linestyle='--')\r\n",
    "    plt.plot(thresholds, f1_scores[:thresholds.shape[0]],  label='f1',        linestyle='solid')\r\n",
    "    # valid linestyle = '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\r\n",
    "\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    return thresholds, precisions, recalls, f1_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ra_curve(y_val, probas_pred):\r\n",
    "\r\n",
    "    \"\"\"ROC curve, auc_score\"\"\"\r\n",
    "    \r\n",
    "    FPRS, TPRS, thresholds = roc_curve(y_val, probas_pred[:, 1])\r\n",
    "\r\n",
    "    # settings\r\n",
    "    plt.title('ROC curve')\r\n",
    "    plt.gray()\r\n",
    "    plt.xlabel('FPR(1- specificity)')\r\n",
    "    plt.ylabel('TPR')\r\n",
    "\r\n",
    "    # x, y values\r\n",
    "    plt.plot(FPRS, TPRS, label='ROC', linestyle='solid')\r\n",
    "    plt.plot([0, 1], [0, 1], label='50%', color='gray', linestyle=':')\r\n",
    "\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    return FPRS, TPRS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def max_eval(y_val, y_pred, probas_pred, \r\n",
    "             thresholds, f1_scores, precisions, recalls, FPRS, TPRS):\r\n",
    "      \r\n",
    "      \"\"\"when f1 max, threshold & scorings\"\"\"\r\n",
    "      \r\n",
    "      print('-' * 35, 'max_eval', '-' * 35)\r\n",
    "\r\n",
    "      f1max_idx = np.where(f1_scores == f1_scores.max())\r\n",
    "      m_threshold = float(thresholds[f1max_idx])\r\n",
    "      m_proba     = Binarizer(threshold=m_threshold).fit_transform(probas_pred[:, 1].reshape(-1, 1))\r\n",
    "\r\n",
    "      max_f1      = f1_scores.max()\r\n",
    "      m_accuracy  = accuracy_score (y_val, m_proba)\r\n",
    "      m_AUC       = roc_auc_score(y_val, probas_pred[:, 1])\r\n",
    "      m_precision = float(precisions[f1max_idx])\r\n",
    "      m_recall    = float(recalls[f1max_idx])\r\n",
    "      m_FPR       = float(FPRS[np.where(thresholds == m_threshold)])\r\n",
    "      m_TPR       = float(TPRS[np.where(thresholds == m_threshold)])\r\n",
    "\r\n",
    "      print(f'threshold = {m_threshold:.4f}   '\r\n",
    "            f'max f1   = {max_f1:.4f}   '\r\n",
    "            f'accuracy = {m_accuracy:.4f}   '\r\n",
    "            f'AUC      = {m_AUC:.4f}\\n'\r\n",
    "            f'precision = {m_precision:.4f}   '\r\n",
    "            f'recall   = {m_recall:.4f}   '\r\n",
    "            f'FPR      = {m_FPR:.4f}   '\r\n",
    "            f'TPR      = {m_TPR:.4f}')    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_eval(th, y_val, y_pred, probas_pred, model, X_val):\r\n",
    "\r\n",
    "    \"\"\"th in th_list scorings\"\"\"\r\n",
    "    \r\n",
    "    print('-' * 29, 'threshold adjusting ', '-' * 29)\r\n",
    "\r\n",
    "    f1        = f1_score        (y_val, y_pred)\r\n",
    "    accuracy  = accuracy_score  (y_val, y_pred)\r\n",
    "    AUC       = roc_auc_score   (y_val, probas_pred[:, 1])\r\n",
    "    precision = precision_score (y_val, y_pred)\r\n",
    "    recall    = recall_score    (y_val, y_pred)\r\n",
    "\r\n",
    "    print(f'threshold = {th:.4f}   '\r\n",
    "          f'f1       = {f1:.4f}   '\r\n",
    "          f'accuracy = {accuracy:.4f}   '\r\n",
    "          f'AUC      = {AUC:.4f}\\n'\r\n",
    "          f'precision = {precision:.4f}   '\r\n",
    "          f'recall   = {recall:.4f}   \\n')\r\n",
    "\r\n",
    "    conf_matx = confusion_matrix(y_val, y_pred)\r\n",
    "    disp = plot_confusion_matrix(model, X_val, y_pred, cmap=plt.cm.Blues, normalize='all')\r\n",
    "    disp.ax_.set_title(th)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_fit_score(X, y, model=RandomForestClassifier(),\r\n",
    "                    test_size=0.2, th_list=[0.5], random_state=0):\r\n",
    "\r\n",
    "    \"\"\"train_test_split & fit_predict\"\"\"\r\n",
    "    \r\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    y_pred = model.predict(X_val)\r\n",
    "    probas_pred = model.predict_proba(X_val)\r\n",
    "    \r\n",
    "    # precision_recall_curve & ROC_curve & max_eval\r\n",
    "    thresholds, precisions, recalls, f1_scores = pr_curve(y_val, probas_pred)\r\n",
    "    FPRS, TPRS = ra_curve(y_val, probas_pred)\r\n",
    "    max_eval(y_val, y_pred, probas_pred, thresholds, f1_scores, precisions, recalls, FPRS, TPRS)\r\n",
    "\r\n",
    "    # binarize loop\r\n",
    "    for th in th_list:\r\n",
    "        bin_probas = Binarizer(threshold=th).fit_transform(probas_pred[:, 1].reshape(-1, 1))\r\n",
    "        my_eval(th, y_val, bin_probas, probas_pred, model, X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_fit_score(X, y, th_list=[0.375, 0.4])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "c3102799280f1ed23787b1be8e33b065cea14d14e60df4daed798ef8bab0b32c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}